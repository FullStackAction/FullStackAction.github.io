(window.webpackJsonp=window.webpackJsonp||[]).push([[182],{591:function(t,s,a){"use strict";a.r(s);var n=a(21),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"一、scrapy-概念和流程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#一、scrapy-概念和流程"}},[t._v("#")]),t._v(" 一、scrapy 概念和流程")]),t._v(" "),s("h3",{attrs:{id:"_1、概念"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1、概念"}},[t._v("#")]),t._v(" 1、概念")]),t._v(" "),s("p",[t._v("Scrapy 是一个 python 编写的，被设计用于爬取网络数据、提取结构性数据的开源网络爬虫框架。")]),t._v(" "),s("ul",[s("li",[t._v("作用：少量的代码，就能够快速的抓取")]),t._v(" "),s("li",[t._v("官方文档："),s("a",{attrs:{href:"https://scrapy-chs.readthedocs.io/zh_CN/0.24/",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://scrapy-chs.readthedocs.io/zh_CN/0.24/"),s("OutboundLink")],1)])]),t._v(" "),s("blockquote",[s("p",[t._v("补充：Scrapy 使用了 Twisted 异步网络框架，可以加快下载速度")])]),t._v(" "),s("h3",{attrs:{id:"_2、工作流程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2、工作流程"}},[t._v("#")]),t._v(" 2、工作流程")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/FullStackAction/PicBed@resource20210320170901/image/20210704154838.jpg",alt:"scrapy流程图"}})]),t._v(" "),s("p",[t._v("其流程描述如下：")]),t._v(" "),s("ol",[s("li",[t._v("爬虫中起始的 url 构造成 request 对象 --\x3e 爬虫中间件 --\x3e 引擎 --\x3e 调度器")]),t._v(" "),s("li",[t._v("调度器把 request --\x3e 引擎 --\x3e 下载中间件 --\x3e 下载器")]),t._v(" "),s("li",[t._v("下载器发送请求，获取 response 响应 --\x3e 下载中间件 --\x3e 引擎 --\x3e 爬虫中间件 --\x3e 爬虫")]),t._v(" "),s("li",[t._v("爬虫提取 url 地址，组装成 request 对象 --\x3e 爬虫中间件 --\x3e 引擎 --\x3e 调度器，重复步骤 2")]),t._v(" "),s("li",[t._v("爬虫提取数据 --\x3e 引擎 --\x3e 管道处理和保存数据")])]),t._v(" "),s("p",[t._v("注意：")]),t._v(" "),s("ul",[s("li",[t._v("图中绿色线条表示数据的传递")]),t._v(" "),s("li",[t._v("图中中间件的位置决定了其作用")]),t._v(" "),s("li",[t._v("引擎处于 4 个模块中间，各个模块之间相互独立，只和引擎进行交互")])]),t._v(" "),s("h3",{attrs:{id:"_3、各模块的具体作用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3、各模块的具体作用"}},[t._v("#")]),t._v(" 3、各模块的具体作用")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("模块")]),t._v(" "),s("th",[t._v("作用")]),t._v(" "),s("th",[t._v("是否实现")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("Scrapy Engine(引擎)")]),t._v(" "),s("td",[t._v("总指挥：负责数据和信号在不同模块之间传递")]),t._v(" "),s("td",[t._v("scrapy 已经实现")])]),t._v(" "),s("tr",[s("td",[t._v("Scheduler(调度器)")]),t._v(" "),s("td",[t._v("一个队列，存放引擎发过来的 request 请求")]),t._v(" "),s("td",[t._v("scrapy 已经实现")])]),t._v(" "),s("tr",[s("td",[t._v("Downloader(下载器)")]),t._v(" "),s("td",[t._v("下载把引擎发过来的 request 请求，并返回给引擎")]),t._v(" "),s("td",[t._v("scrapy 已经实现")])]),t._v(" "),s("tr",[s("td",[t._v("Spider(爬虫)")]),t._v(" "),s("td",[t._v("处理引擎发来的 response，提取数据、url，并交给引擎")]),t._v(" "),s("td",[s("strong",[s("em",[t._v("需要手写")])])])]),t._v(" "),s("tr",[s("td",[t._v("Item Pipeline(管道)")]),t._v(" "),s("td",[t._v("处理引擎传过来的数据，比如存储")]),t._v(" "),s("td",[s("strong",[s("em",[t._v("需要手写")])])])]),t._v(" "),s("tr",[s("td",[t._v("Downloader Middlewares(下载中间件)")]),t._v(" "),s("td",[t._v("可以自定义的下载扩展，比如设置代理")]),t._v(" "),s("td",[t._v("一般不用手写")])]),t._v(" "),s("tr",[s("td",[t._v("Spider Middlewares(爬虫中间件)")]),t._v(" "),s("td",[t._v("可以自定义 requests 请求和进行 response 过滤")]),t._v(" "),s("td",[t._v("一般不用手写")])])])]),t._v(" "),s("blockquote",[s("p",[t._v("注意：爬虫中间件 和 下载中间件 只是运行逻辑的位置不同，作用是重复的：如替换 UA 等。")])]),t._v(" "),s("h2",{attrs:{id:"二、scrapy-入门使用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#二、scrapy-入门使用"}},[t._v("#")]),t._v(" 二、scrapy 入门使用")]),t._v(" "),s("h3",{attrs:{id:"_1、安装-scrapy"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1、安装-scrapy"}},[t._v("#")]),t._v(" 1、安装 scrapy")]),t._v(" "),s("p",[t._v("scrapy 有 2 种安装方式：")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("命令：")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[s("span",{pre:!0,attrs:{class:"token function"}},[t._v("sudo")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("apt-get")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" scrapy\n")])])])]),t._v(" "),s("li",[s("p",[t._v("pip：")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("pip/pip3 "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" scrapy\n")])])])])]),t._v(" "),s("h3",{attrs:{id:"_2、项目开发流程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2、项目开发流程"}},[t._v("#")]),t._v(" 2、项目开发流程")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("创建项目：")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("scrapy startproject "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("项目名称"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n\neg:\n\tscrapy startproject myspider\n")])])])]),t._v(" "),s("li",[s("p",[t._v("生成一个爬虫：")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("scrapy genspider "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("爬虫名字"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("允许爬取的域名"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\n\neg:\n\t"),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("cd")]),t._v(" myspider\n\tscrapy genspider example example.com\n")])])]),s("ul",[s("li",[s("p",[t._v("爬虫名字：作用爬虫运行时的参数")])]),t._v(" "),s("li",[s("p",[t._v("允许爬取的域名：为对于爬虫设置的爬取范围，设置之后用于过滤要爬取的 url，如果爬取的 url 与允许的域不同则被过滤掉")]),t._v(" "),s("blockquote",[s("p",[t._v("注意：执行完命令后，myspider/spiders 目录下，会多出一个 example.py 爬虫文件")])])])])]),t._v(" "),s("li",[s("p",[t._v("提取数据：")]),t._v(" "),s("p",[t._v("根据网站结构在 spider 中实现数据采集相关内容")])]),t._v(" "),s("li",[s("p",[t._v("保存数据：")]),t._v(" "),s("p",[t._v("使用 pipeline 进行数据后续处理和保存")])]),t._v(" "),s("li",[s("p",[t._v("运行 scrapy")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("scrapy crawl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("爬虫名字"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("\nscrapy crawl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v("爬虫名字"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("--nolog")]),t._v("\n\neg:\n\tscrapy crawl example\n")])])]),s("blockquote",[s("p",[t._v("注意 ：需要在项目录下执行命令；"),s("code",[t._v("--nolog")]),t._v(" 可以关闭框架日志信息输出")])])])]),t._v(" "),s("h3",{attrs:{id:"_3、三个内置对象"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3、三个内置对象"}},[t._v("#")]),t._v(" 3、三个内置对象")]),t._v(" "),s("ul",[s("li",[t._v("request 请求对象：由 url method post_data headers 等构成")]),t._v(" "),s("li",[t._v("response 响应对象：由 url body status headers 等构成")]),t._v(" "),s("li",[t._v("item 数据对象：本质是字典")])]),t._v(" "),s("h3",{attrs:{id:"_1-定位元素以及提取数据"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-定位元素以及提取数据"}},[t._v("#")]),t._v(" 1）定位元素以及提取数据")]),t._v(" "),s("p",[t._v("解析并获取 scrapy 爬虫中的数据：利用 xpath 规则字符串进行定位和提取")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("response.xpath()")]),t._v("：返回一个类似 List 的类型，其中包含的是 selector 对象，操作和列表一样，但是有一些额外方法：\n"),s("ul",[s("li",[s("code",[t._v("extract()")]),t._v("：返回一个包含有字符串的列表")]),t._v(" "),s("li",[s("code",[t._v("extract_first()")]),t._v("：返回列表中的第一个字符串，列表为空则返回 None")])])])]),t._v(" "),s("h3",{attrs:{id:"_2-response-响应对象的常用属性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-response-响应对象的常用属性"}},[t._v("#")]),t._v(" 2）response 响应对象的常用属性")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("response.url")]),t._v("：当前响应的 url 地址")]),t._v(" "),s("li",[s("code",[t._v("response.request.url")]),t._v("：当前响应对应的请求的 url 地址")]),t._v(" "),s("li",[s("code",[t._v("response.headers")]),t._v("：响应头")]),t._v(" "),s("li",[s("code",[t._v("response.request.headers")]),t._v("：当前响应的请求头")]),t._v(" "),s("li",[s("code",[t._v("response.body")]),t._v("：响应体，也就是 html 代码，byte 类型")]),t._v(" "),s("li",[s("code",[t._v("response.status")]),t._v("：响应状态码")])]),t._v(" "),s("h3",{attrs:{id:"_4、入门实战"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4、入门实战"}},[t._v("#")]),t._v(" 4、入门实战")]),t._v(" "),s("h4",{attrs:{id:"_1-创建项目-爬虫"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-创建项目-爬虫"}},[t._v("#")]),t._v(" 1）创建项目&爬虫")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("scrapy startproject myspider\n"),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("cd")]),t._v(" myspider\nscrapy genspider itcast itcast.cn\n")])])]),s("h4",{attrs:{id:"_2-完善爬虫-抓取数据"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-完善爬虫-抓取数据"}},[t._v("#")]),t._v(" 2）完善爬虫（抓取数据）")]),t._v(" "),s("p",[t._v("在/myspider/myspider/spiders/itcast.py 中修改内容如下：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scrapy\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ItcastSpider")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'itcast'")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.检查域名")]),t._v("\n    allowed_domains "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'itcast.cn'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1.修改起始url")]),t._v("\n    start_urls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'http://itcast.cn/channel/teacher.shtml'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 设置起始的url，我们只需要设置就好，通常会被自动的创建成请求发送")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3.在parse方法中实现爬取逻辑")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 解析方法，通常用于起始url对应响应的解析")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 定义对于网站的相关操作")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# with open('itcast.html', 'wb') as f:")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     f.write(response.body)")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取所有老师节点")]),t._v("\n        node_list "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'//div[@class=\"li_txt\"]'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 遍历老师节点列表")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" node "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" node_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            temp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# xpath方法返回的是选择器对象列表，extract()用于从选择器对象中提取数据")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# temp['name'] = node.xpath('./h3/text()')[0].extract()")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# temp['title'] = node.xpath('./h4/text()')[0].extract()")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# temp['desc'] = node.xpath('./p/text()')[0].extract()")]),t._v("\n\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# xpath结果为只含有一个值的列表，可以使用extract_first()，如果为多个值则使用extract()")]),t._v("\n            temp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'name'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./h3/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            temp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'title'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./h4/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            temp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'desc'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./p/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# {'name': [<Selector xpath='./h3/text()' data='黄老师'>], 'title': [<Selector xpath='./h4/text()' data='高级讲师'>], 'desc': [<Selector xpath='./p/text()' data='15年+的软件开发与教学经验，参与中国联通VOP系统，中国联通在线计费系统...'>]}")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# extract()之后：")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# {'name': '黄老师', 'title': '高级讲师', 'desc': '15年+的软件开发与教学经验，参与中国联通VOP系统，中国联通在线计费系统，道路交通事故救助基金管理系统等的开发工作；\\r\\n拥有丰富的教学经验。'}")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("temp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" temp\n")])])]),s("p",[t._v("注意：")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("scrapy.Spider")]),t._v("爬虫类中必须有名为 parse 的解析")]),t._v(" "),s("li",[t._v("如果网站结构层次比较复杂，也可以自定义其他解析函数")]),t._v(" "),s("li",[t._v("在解析函数中提取的 url 地址如果要发送请求，则必须属于 "),s("code",[t._v("allowed_domains")]),t._v(" 范围内，但是 "),s("code",[t._v("start_urls")]),t._v(" 中的 url 地址不受这个限制")]),t._v(" "),s("li",[s("code",[t._v("parse()")]),t._v(" 函数中使用 "),s("code",[t._v("yield")]),t._v(" 返回数值。")]),t._v(" "),s("li",[s("strong",[t._v("yield 能够传递的对象只能是：BaseItem，Request，dict，None")])])]),t._v(" "),s("h4",{attrs:{id:"_3-完善管道-保存数据"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-完善管道-保存数据"}},[t._v("#")]),t._v(" 3）完善管道（保存数据）")]),t._v(" "),s("ol",[s("li",[t._v("定义一个管道类")]),t._v(" "),s("li",[t._v("重写管道类的 "),s("code",[t._v("process_item()")]),t._v(" 方法")]),t._v(" "),s("li",[s("code",[t._v("process_item()")]),t._v(" 方法处理完 item 之后必须返回给引擎")])]),t._v(" "),s("p",[t._v("这里直接使用创建项目时默认创建好的管道类：/myspider/myspider/pipelines/MyspiderPipeline，修改内容如下：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" json\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MyspiderPipeline")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'itcast.json'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("process_item")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print('item:', item)")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将字典数据序列化")]),t._v("\n        json_data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dumps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ensure_ascii"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("',\\n'")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将数据写入文件")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("json_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 默认使用完管道之后需要将数据返回给引擎")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" item\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__del__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h4",{attrs:{id:"_4-启用管道"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-启用管道"}},[t._v("#")]),t._v(" 4）启用管道")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("ITEM_PIPELINES "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n   "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'myspider.pipelines.MyspiderPipeline'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("300")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 'myspider.pipelines.MyspiderPipelineOther': 301,")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("ul",[s("li",[t._v("配置项中，键为使用的管道类，管道类引用声明使用"),s("code",[t._v(".")]),t._v("进行分割，分别为："),s("code",[t._v("项目目录.文件.管理类")]),t._v("。")]),t._v(" "),s("li",[t._v("配置项中，值为管道的使用顺序，设置的数值越小越先执行，该值一般设置在 1000 以内。")])]),t._v(" "),s("h4",{attrs:{id:"_5-运行爬虫"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-运行爬虫"}},[t._v("#")]),t._v(" 5）运行爬虫")]),t._v(" "),s("p",[t._v("在控制台运行如下命令即可运行爬虫：")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("scrapy crawl itcast\nscrapy crawl itcast "),s("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("--nolog")]),t._v("\n")])])]),s("h2",{attrs:{id:"三、scrapy-数据建模-items"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#三、scrapy-数据建模-items"}},[t._v("#")]),t._v(" 三、scrapy 数据建模(items)")]),t._v(" "),s("h3",{attrs:{id:"_1、数据建模"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1、数据建模"}},[t._v("#")]),t._v(" 1、数据建模")]),t._v(" "),s("p",[t._v("通常在项目开发过程中，需要在 "),s("code",[t._v("items.py")]),t._v(" 中进行数据建模")]),t._v(" "),s("h4",{attrs:{id:"_1-为什么要建模"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-为什么要建模"}},[t._v("#")]),t._v(" 1）为什么要建模")]),t._v(" "),s("ul",[s("li",[t._v("定义 item 即提前规划好哪些字段需要抓取，防止手误，因为定义好之后，在运行过程中，系统会自动检查")]),t._v(" "),s("li",[t._v("配置注释一起可以清晰的知道要抓取哪些字段，没有定义的字段不能抓取，在目标字段少的时候可以使用字典代替")]),t._v(" "),s("li",[t._v("使用 scrapy 的一些特定组件中需要 Item 做支持，如 scrapy 的 ImagesPipeline 管道类。")])]),t._v(" "),s("h4",{attrs:{id:"_2-如何建模"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-如何建模"}},[t._v("#")]),t._v(" 2）如何建模")]),t._v(" "),s("p",[t._v("在 "),s("code",[t._v("items.py")]),t._v(" 文件中定义发提取的字段：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scrapy\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MyspiderItem")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# define the fields for your item here like:")]),t._v("\n    name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 讲师的名字")]),t._v("\n    title "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 讲师的职称")]),t._v("\n    desc "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 讲师的介绍")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# if __name__ == '__main__':")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     item = MyspiderItem()")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     item['name'] = 'lqr'  # ok")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     item['nama'] = 'lqr'  # KeyError: 'MyspiderItem does not support field: nama'")]),t._v("\n")])])]),s("blockquote",[s("p",[t._v("注意："),s("code",[t._v("scrapy.Item")]),t._v(" 可以理解为一个更高级的 "),s("strong",[t._v("“字典”")]),t._v("，可以对键名进行限制、校验。但切记它不是字典，如果你需要对字典进行操作，可以使用 "),s("code",[t._v("dict()")]),t._v(" 将"),s("code",[t._v("scrapy.Item")]),t._v(" 进行强制转换。")])]),t._v(" "),s("h4",{attrs:{id:"_3-使用模板类"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-使用模板类"}},[t._v("#")]),t._v(" 3）使用模板类")]),t._v(" "),s("p",[t._v("模板类定义以后，需要在爬虫中导入并且实例化，之后的使用方法和字典相同：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果报包报错有2种解决办法：")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1、PyCharm以myspider为根目录重新打开项目")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2、不以myspider为根目录的话，可以右键myspider--\x3eMake Directory as--\x3eSources Root")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" myspider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" MyspiderItem\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        item "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" MyspiderItem"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n        item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'name'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./h3/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'title'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./h4/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'desc'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./p/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" item\n")])])]),s("h4",{attrs:{id:"_4-开发流程总结"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-开发流程总结"}},[t._v("#")]),t._v(" 4）开发流程总结")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("创建项目")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("scrapy startproject 项目名\n")])])])]),t._v(" "),s("li",[s("p",[t._v("明确目标")]),t._v(" "),s("ul",[s("li",[t._v("在 items.py 文件中进行建模")])])]),t._v(" "),s("li",[s("p",[t._v("创建爬虫")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("创建爬虫")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("scrapy genspider 爬虫名 允许的域\n")])])])]),t._v(" "),s("li",[s("p",[t._v("编写爬虫")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("修改start_urls\n检查修改allowed_domains\n编写解析方法\n")])])])])])]),t._v(" "),s("li",[s("p",[t._v("保存数据")]),t._v(" "),s("ul",[s("li",[t._v("在 "),s("code",[t._v("pipelines.py")]),t._v(" 文件中定义对数据处理的管道")]),t._v(" "),s("li",[t._v("在 "),s("code",[t._v("settings.py")]),t._v(" 文件中注册启用管道")])])])]),t._v(" "),s("h2",{attrs:{id:"四、scrapy-处理翻页-request"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#四、scrapy-处理翻页-request"}},[t._v("#")]),t._v(" 四、scrapy 处理翻页(Request)")]),t._v(" "),s("h3",{attrs:{id:"_1、思路"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1、思路"}},[t._v("#")]),t._v(" 1、思路")]),t._v(" "),s("ol",[s("li",[t._v("找到下一页的 url 地址")]),t._v(" "),s("li",[t._v("构造 url 地址的请求对象，传递给引擎")])]),t._v(" "),s("h3",{attrs:{id:"_2、实现步骤"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2、实现步骤"}},[t._v("#")]),t._v(" 2、实现步骤")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("确定 url 地址")])]),t._v(" "),s("li",[s("p",[t._v("构造请求")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Request"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" callback"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("ul",[s("li",[s("code",[t._v("callback")]),t._v("：指定解析函数名称，表示该请求返回的响应使用哪一个函数进行解析")])])]),t._v(" "),s("li",[s("p",[t._v("把请求交给引擎")])])]),t._v(" "),s("h3",{attrs:{id:"_3、实战-网易招聘爬虫"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3、实战-网易招聘爬虫"}},[t._v("#")]),t._v(" 3、实战（网易招聘爬虫）")]),t._v(" "),s("h4",{attrs:{id:"_1-思路分析"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-思路分析"}},[t._v("#")]),t._v(" 1）思路分析")]),t._v(" "),s("ol",[s("li",[t._v("获取首页数据")]),t._v(" "),s("li",[t._v("寻找下一页地址，进行翻页，获取数据")])]),t._v(" "),s("h4",{attrs:{id:"_2-代码实现"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-代码实现"}},[t._v("#")]),t._v(" 2）代码实现")]),t._v(" "),s("ul",[s("li",[t._v("创建项目")])]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("scrapy startproject wangyi\n\n"),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("cd")]),t._v(" wangyi\nscrapy gespider job "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("163")]),t._v(".com\n")])])]),s("ul",[s("li",[t._v("定义模板类")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scrapy\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WangyiItem")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# define the fields for your item here like:")]),t._v("\n    name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    link "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    depart "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    category "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    address "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    num "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Field"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("ul",[s("li",[t._v("编写爬虫（抓取数据）")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scrapy\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" wangyi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" WangyiItem\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JobSpider")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job'")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.检查修改allowed_domains")]),t._v("\n    allowed_domains "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'163.com'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1.修改start_urls")]),t._v("\n    start_urls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'https://hr.163.com/position/list.do'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 提取数据")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取所有职位节点列表")]),t._v("\n        node_list "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'//*[@class=\"position-tb\"]/tbody/tr'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 遍历节点列表")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" num"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" node "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("node_list"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 设置过滤条件，将目标节点获取出来")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" num "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                item "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" WangyiItem"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'name'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./td[1]/a/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# item['link'] = 'https://hr.163.com' + node.xpath('./td[1]/a/@href').extract_first()")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# response.urljoin()用于拼接相对路径的url，可以理解为自动补全")]),t._v("\n                item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'link'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("urljoin"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./td[1]/a/@href'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'depart'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./td[2]/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'category'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./td[3]/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'type'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./td[4]/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'address'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./td[5]/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'num'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./td[6]/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("strip"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'date'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" node"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./td[7]/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(item)")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" item\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 模拟翻页")]),t._v("\n        part_url "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/html/body/div[2]/div[2]/div[2]/div/a[last()]/@href'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 判断终止条件")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" part_url "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'javascript:void(0)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            next_url "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("urljoin"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("part_url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 构建请求对象，并且返回给引擎")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Request"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                url"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("next_url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                callback"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse\n            "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("blockquote",[s("p",[t._v("注意：引擎根据爬虫 "),s("code",[t._v("yield")]),t._v(" 的对象类型，将对象分配给对应的模块处理，比如：如果是模板类对象或字典，则会交给管道(Item Pipeline)；如果是 Request 对象，则会交给队列(Scheduler)。")])]),t._v(" "),s("ul",[s("li",[t._v("编写管道（存储数据）")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" json\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WangyiPipeline")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'wangyi.json'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("process_item")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        item "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        str_data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dumps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ensure_ascii"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("',\\n'")]),t._v("\n\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("str_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" item\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__del__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("ul",[s("li",[t._v("启用管道")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("ITEM_PIPELINES "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'wangyi.pipelines.WangyiPipeline'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("300")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("ul",[s("li",[t._v("运行爬虫")])]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("scrapy crawl job "),s("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("--nolog")]),t._v("\n")])])]),s("h4",{attrs:{id:"_3-扩展"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-扩展"}},[t._v("#")]),t._v(" 3）扩展：")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("可以在 settings 中设置 ROBOTS 协议")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# False表示忽略网站的robots.txt协议，默认为True")]),t._v("\nROBOTSTXT_OBEY "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n")])])])]),t._v(" "),s("li",[s("p",[t._v("可以在 settings 中设置 User-Agent：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# scrapy发送的每一个请求的默认UA都是设置的这个User-Agent")]),t._v("\nUSER_AGENT "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'")]),t._v("\n")])])])])]),t._v(" "),s("h3",{attrs:{id:"_4、scrapy-request-的更多参数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4、scrapy-request-的更多参数"}},[t._v("#")]),t._v(" 4、scrapy.Request 的更多参数")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Request"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" callback"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" method"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"GET"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" headers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" body"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cookies"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" meta"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dont_filter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("blockquote",[s("p",[t._v("注意：中括号[]里的参数为可选参数")])]),t._v(" "),s("p",[t._v("参数解释：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("callback")]),t._v("：表示当前 url 的响应交给哪个函数去处理")]),t._v(" "),s("li",[s("strong",[t._v("meta")]),t._v("：实现数据在不同的解析函数中传递，meta 默认带有部分数据，比如下载延迟，请求深度等")]),t._v(" "),s("li",[t._v("dont_filter：默认为 False，会过滤请求的 url 地址，即请求过的 url 地址不会继续被请求，对需要重复请求的 url 地址可以把它设置为 True，比如贴吧的翻页请求，页面的数据总是在变化，start_urls 中的地址会被反复请求，否则程序不会启动")]),t._v(" "),s("li",[t._v("method：指定 POST 或 GET 请求")]),t._v(" "),s("li",[t._v("headers：接收一个字典，其中不包括 cookies")]),t._v(" "),s("li",[t._v("cookies：接收一个字典，专门放置 cookies")]),t._v(" "),s("li",[t._v("body：接收 json 字符串，为 POST 的数据，发送 payload_post 请求时使用")])]),t._v(" "),s("h4",{attrs:{id:"_1-meta-参数的使用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-meta-参数的使用"}},[t._v("#")]),t._v(" 1）meta 参数的使用")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("meta 的使用：")]),t._v(" "),s("ul",[s("li",[t._v("meta 可以实现数据在不同的解析函数中传递")])])]),t._v(" "),s("li",[s("p",[t._v("meta 的注意事项：")]),t._v(" "),s("ul",[s("li",[t._v("meta 参数是一个字典")]),t._v(" "),s("li",[t._v("meta 字典中有一个固定的键 proxy，表示代理 ip")])])]),t._v(" "),s("li",[s("p",[t._v("使用举例：")])])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Request"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("detail_url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" callback"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse_detail"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" meta"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'item'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse_detail")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 获取之前传入的item")]),t._v("\n    item "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("meta"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'item'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("h2",{attrs:{id:"五、scrapy-模拟登录-cookie"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#五、scrapy-模拟登录-cookie"}},[t._v("#")]),t._v(" 五、scrapy 模拟登录(cookie)")]),t._v(" "),s("h3",{attrs:{id:"_1、模拟登录的不同实现方式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1、模拟登录的不同实现方式"}},[t._v("#")]),t._v(" 1、模拟登录的不同实现方式")]),t._v(" "),s("ul",[s("li",[t._v("requests 模块\n"),s("ul",[s("li",[t._v("直接携带 cookies 请求页面")]),t._v(" "),s("li",[t._v("找到 url 地址，发送 post 请求存储 cookie")])])]),t._v(" "),s("li",[t._v("selenium（浏览器自动处理 cookie）\n"),s("ul",[s("li",[t._v("找到对应的 input 标签，输入文本点击登录")])])]),t._v(" "),s("li",[t._v("scrapy\n"),s("ul",[s("li",[t._v("直接携带 cookies")]),t._v(" "),s("li",[t._v("找 url 地址，发送 post 请求存储 cookie")])])])]),t._v(" "),s("h3",{attrs:{id:"_2、scrapy-直接携带-cookies-获取需要登录信息的页面"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2、scrapy-直接携带-cookies-获取需要登录信息的页面"}},[t._v("#")]),t._v(" 2、scrapy 直接携带 cookies 获取需要登录信息的页面")]),t._v(" "),s("p",[t._v("应用场景：")]),t._v(" "),s("ul",[s("li",[t._v("cookie 过期时间很长，常见于一些不规范的网站")]),t._v(" "),s("li",[t._v("能在 cookie 过期之前把所有的数据拿到")]),t._v(" "),s("li",[t._v("配合其他程序使用，比如其使用 selenium 把登录之后的 cookie 获取到并保存到本地，scrapy 发送请求之前先读取本地 cookie")])]),t._v(" "),s("h4",{attrs:{id:"_1、start-url-携带-cookie-重写-start-requests-方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1、start-url-携带-cookie-重写-start-requests-方法"}},[t._v("#")]),t._v(" 1、start_url 携带 cookie（重写 start_requests 方法）")]),t._v(" "),s("p",[t._v("scrapy 中 start_url 是通过 start_requests 来进行处理的，可能通过重写该方法让 start_url 携带上请求头信息，实现代码如下：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scrapy\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Git1Spider")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'git1'")]),t._v("\n    allowed_domains "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'github.com'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    start_urls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'https://github.com/GitLqr'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("start_requests")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n        重写start_requests，发送携带cookies的Request。\n        默认start_requests只是普通的get请求，不会携带自定义的头信息\n        """')]),t._v("\n        url "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start_urls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n        temp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_octo=GH1.1.1045146750.1615451260; _device_id=cd8d64981fcb3fd4ba7f587873e97804'")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 把cookies字符串转成字典")]),t._v("\n        cookies "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'='")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'='")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" data "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" temp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'; '")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Request"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            url"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            callback"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            cookies"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cookies\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/html/head/title/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("注意：")]),t._v(" "),s("ul",[s("li",[t._v("scrapy 中 cookie 不能够放在 headers 中，在构造请求的时候有专门的 cookies 参数，能够接收字典形式的 cookie")]),t._v(" "),s("li",[t._v("可能需要在 settings 中设置 ROBOTS 协议、USER_AGENT")])]),t._v(" "),s("h3",{attrs:{id:"_2、scrapy-request-发送-post-请求"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2、scrapy-request-发送-post-请求"}},[t._v("#")]),t._v(" 2、scrapy.Request 发送 post 请求")]),t._v(" "),s("p",[t._v("scrapy.Request 发送 post 请求有两种方式：")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("通过 "),s("code",[t._v("scrapy.Request()")]),t._v(" 指定 method、body 参数来发送 post 请求（不推荐）")])]),t._v(" "),s("li",[s("p",[t._v("使用 "),s("code",[t._v("scrapy.FormRequest()")]),t._v(" 来发送 post 请求（推荐）")])])]),t._v(" "),s("blockquote",[s("p",[t._v("注意："),s("code",[t._v("scrapy.FormRequest()")]),t._v(" 能够发送表单和 ajax 请求，参考阅读 "),s("a",{attrs:{href:"https://www.jb51.net/article/146769.htm",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.jb51.net/article/146769.htm"),s("OutboundLink")],1)])]),t._v(" "),s("p",[t._v("举例：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scrapy\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Git2Spider")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'git2'")]),t._v("\n    allowed_domains "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'github.com'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    start_urls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'http://github.com/login'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        username "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'GitLqr'")]),t._v("\n        password "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'balabala'")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 从登录页面响应中解析出post数据")]),t._v("\n        token "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'//input[@name=\"authenticity_token\"]/@value'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        post_data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'commit'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Sign in'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'authenticity_token'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" token"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'login'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" username"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'password'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" password"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'webauthn-support'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'supported'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("post_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 针对登录url发送post请求")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("FormRequest"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            url"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'https://github.com/session'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            callback"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("after_login"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            formdata"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("post_data\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("after_login")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Request"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'https://github.com/GitLqr'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" callback"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check_login"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("check_login")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/html/head/title/text()'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extract_first"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("blockquote",[s("p",[t._v("注意：在 settings.py 中通过设置 "),s("code",[t._v("COOKIES_DEBUG = True")]),t._v(" 能够在终端查看到 cookie 的传递过程")])]),t._v(" "),s("h2",{attrs:{id:"六、scrapy-管道的使用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#六、scrapy-管道的使用"}},[t._v("#")]),t._v(" 六、scrapy 管道的使用")]),t._v(" "),s("h3",{attrs:{id:"_1、pipeline-中常用的方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1、pipeline-中常用的方法"}},[t._v("#")]),t._v(" 1、pipeline 中常用的方法")]),t._v(" "),s("ul",[s("li",[s("code",[t._v("process_item(self, item, spider)")]),t._v("：\n"),s("ul",[s("li",[t._v("管道类中必须有的函数")]),t._v(" "),s("li",[t._v("实现对 item 数据的处理")]),t._v(" "),s("li",[t._v("必须 "),s("code",[t._v("return item")])])])]),t._v(" "),s("li",[s("code",[t._v("open_spider(self, spider)")]),t._v("：在爬虫开启的时候仅执行一次")]),t._v(" "),s("li",[s("code",[t._v("close_spider(self, spider)")]),t._v("：在爬虫关闭的时候仅执行一次")])]),t._v(" "),s("blockquote",[s("p",[t._v("注意：以上 3 个方法，可以通过 "),s("code",[t._v("spider.name")]),t._v(" 获取爬虫的名字")])]),t._v(" "),s("h3",{attrs:{id:"_2、pipeline-判断数据来源"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2、pipeline-判断数据来源"}},[t._v("#")]),t._v(" 2、pipeline 判断数据来源")]),t._v(" "),s("p",[t._v("scrapy 的 "),s("code",[t._v("Item Pipeline")]),t._v(" 模块可以有多个管道，当有一个 spider 把数据对象通过引擎交给 "),s("code",[t._v("Item Pipeline")]),t._v(" 模块时， "),s("code",[t._v("Item Pipeline")]),t._v(" 模块中的所有管道会按 "),s("code",[t._v("settings.py")]),t._v(" 中指定的管道顺序一一被执行。但很多时候，我们需要管道针对特定爬虫做数据存储的，这时就需要在管道中对数据对象的来源做判断。")]),t._v(" "),s("blockquote",[s("p",[t._v("注意：不同的 pipeline 能够对一个或多个爬虫进行不同的数据处理的操作，比如一个进行数据清洗，一个进行数据的保存")])]),t._v(" "),s("p",[t._v("举例：")]),t._v(" "),s("ul",[s("li",[t._v("爬虫 1：job.py")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scrapy\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" wangyi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" WangyiItem\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JobSpider")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job'")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" item\n")])])]),s("ul",[s("li",[t._v("爬虫 2：job_simple.py")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" scrapy\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" wangyi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" WangyiSimpleItem\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JobSimpleSpider")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job_simple'")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("parse")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" response"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("yield")]),t._v(" item\n")])])]),s("ul",[s("li",[t._v("2 个管道：pipelines.py")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WangyiPipeline")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("open_spider")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'wangyi.json'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("process_item")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            item "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            str_data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dumps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ensure_ascii"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("',\\n'")]),t._v("\n            self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("str_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" item\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("close_spider")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WangyiSimplePipeline")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("open_spider")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job_simple'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'wangyi_simple.json'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("process_item")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job_simple'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            item "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            str_data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dumps"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ensure_ascii"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("',\\n'")]),t._v("\n            self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("str_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" item\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("close_spider")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job_simple'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("file")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"_3、pipeline-使用注意点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3、pipeline-使用注意点"}},[t._v("#")]),t._v(" 3、pipeline 使用注意点")]),t._v(" "),s("ul",[s("li",[t._v("使用之前需要在 settings 中开启")]),t._v(" "),s("li",[t._v("pipeline 在 settings 中键表示位置（即 pipeline 在项目中的位置可以自定义），值表示距离引擎的远近，越近数据会越先经过："),s("strong",[t._v("权重值小的优先执行")])]),t._v(" "),s("li",[t._v("有多个 pipeline 的时候，"),s("code",[t._v("process_item")]),t._v(" 的方法必须 "),s("code",[t._v("return item")]),t._v("，否则后一个 pipeline 取到的数据为 None 值")]),t._v(" "),s("li",[t._v("pipeline 中 "),s("code",[t._v("process_item")]),t._v(" 方法必须要有，否则 item 没有办法接收和处理")]),t._v(" "),s("li",[s("code",[t._v("process_item")]),t._v(" 方法接收 item 和 spider，其中 spider 表示当前传递 item 过来的 spider")]),t._v(" "),s("li",[s("code",[t._v("open_spider(spider)")]),t._v("：能够在爬虫开启的时候执行一次")]),t._v(" "),s("li",[s("code",[t._v("close_spider(spider)")]),t._v("：能够在爬虫关闭的时候执行一次")]),t._v(" "),s("li",[t._v("上述俩个方法经常用于爬虫和数据库的交互，在爬虫开启的时候建立和数据库的连接，在爬虫关闭的时候断开和数据库的连接")])]),t._v(" "),s("h2",{attrs:{id:"七、scrapy-中间件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#七、scrapy-中间件"}},[t._v("#")]),t._v(" 七、scrapy 中间件")]),t._v(" "),s("h3",{attrs:{id:"_1、介绍"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1、介绍"}},[t._v("#")]),t._v(" 1、介绍")]),t._v(" "),s("h4",{attrs:{id:"_1-分类"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-分类"}},[t._v("#")]),t._v(" 1）分类")]),t._v(" "),s("p",[t._v("根据 scrapy 运行流程中所在位置不同，对 scrapy 中间件进行分类：")]),t._v(" "),s("ul",[s("li",[t._v("下载中间件")]),t._v(" "),s("li",[t._v("爬虫中间件")])]),t._v(" "),s("h4",{attrs:{id:"_2-作用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-作用"}},[t._v("#")]),t._v(" 2）作用")]),t._v(" "),s("p",[t._v("scrapy 中间件的作用是：预处理 request 和 response 对象")]),t._v(" "),s("ul",[s("li",[t._v("对 header 以及 cookie 进行更换和处理")]),t._v(" "),s("li",[t._v("使用代理 ip 等")]),t._v(" "),s("li",[t._v("对请求进行定制化操作")])]),t._v(" "),s("h4",{attrs:{id:"_3-比较"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-比较"}},[t._v("#")]),t._v(" 3）比较")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("默认情况下，两种中间件都在 "),s("code",[t._v("middlewares.py")]),t._v(" 一个文件中")])]),t._v(" "),s("li",[s("p",[t._v("爬虫中间件使用方法和下载中间件相同，且功能重复，"),s("strong",[t._v("通常使用下载中间件")])])])]),t._v(" "),s("h3",{attrs:{id:"_2、使用方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2、使用方法"}},[t._v("#")]),t._v(" 2、使用方法")]),t._v(" "),s("p",[t._v("注意：结合 scrapy 流程图可方便理解")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/FullStackAction/PicBed@resource20210320170901/image/20210704154838.jpg",alt:"scrapy流程图"}})]),t._v(" "),s("p",[t._v("中间件使用步骤如下：")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("在 "),s("code",[t._v("middlerware.py")]),t._v(" 中定义中间件类")])]),t._v(" "),s("li",[s("p",[t._v("在中间件类中，重写处理请求或者响应的方法（以"),s("code",[t._v("Downloader Middlewares")]),t._v("为例）")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("process_request(self, request, spider)")]),t._v(" "),s("blockquote",[s("p",[t._v("当每个 request 通过下载中间件时，该方法被调用【Scrapy Engine --\x3e Downloader】")])]),t._v(" "),s("ul",[s("li",[t._v("返回 None【继续】：该 request 对象通过引擎传递给其他权重低的 "),s("code",[t._v("process_request")]),t._v(" 方法，如果所有中间件都返回 None，则请求最终被交给下载器处理。注意：没有 return 也是返回 None。")]),t._v(" "),s("li",[t._v("返回 Request 对象【中断】：把 request 对象通过引擎交给调度器，此时将不通过其他权重低的 "),s("code",[t._v("process_request")]),t._v(" 方法")]),t._v(" "),s("li",[t._v("返回 Response 对象【中断】："),s("strong",[t._v("请求不会达到下载器")]),t._v("，会直接把 response 通过引擎交给 Spider 进行解析")])])]),t._v(" "),s("li",[s("p",[t._v("process_response(self, request, response, spider)")]),t._v(" "),s("blockquote",[s("p",[t._v("当下载器完成 http 请求，传递响应给引擎的时候调用【Scrapy Engine <-- Downloader】")])]),t._v(" "),s("ul",[s("li",[t._v("返回 Request 对象【中断】：通过引擎交给调度器继续请求，此时将不通过其他权重低的 "),s("code",[t._v("process_response")]),t._v(" 方法")]),t._v(" "),s("li",[t._v("返回 Response 对象【继续】：通过引擎交给爬虫处理或交给权重更低的其他下载中间件的 "),s("code",[t._v("process_response")]),t._v(" 方法")])])])])]),t._v(" "),s("li",[s("p",[t._v("在 "),s("code",[t._v("settings")]),t._v(" 文件中开启中间件，权重值越小越优先执行（同管道注册一样）")])])]),t._v(" "),s("h3",{attrs:{id:"_3、实现随机-user-agent-下载中间件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3、实现随机-user-agent-下载中间件"}},[t._v("#")]),t._v(" 3、实现随机 User-Agent 下载中间件")]),t._v(" "),s("h4",{attrs:{id:"_1-准备-ua-列表"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-准备-ua-列表"}},[t._v("#")]),t._v(" 1）准备 UA 列表")]),t._v(" "),s("p",[t._v("在 "),s("code",[t._v("settings.py")]),t._v(" 文件中定义一个存放了大量 UA 的列表 "),s("code",[t._v("USER_AGENT_LIST")]),t._v("：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("USER_AGENT "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.72 Safari/537.36'")]),t._v("\n\nUSER_AGENT_LIST "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (iPod; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (iPad; U; CPU OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (Linux; U; Android 2.3.7; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GRJ22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Opera/9.80 (Android 2.3.4; Linux; Opera Mobi/build-1107180945; U; en-GB) Presto/2.8.149 Version/11.10'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (Linux; U; Android 3.0; en-us; Xoom Build/HRI39) AppleWebKit/534.13 (KHTML, like Gecko) Version/4.0 Safari/534.13'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (BlackBerry; U; BlackBerry 9800; en) AppleWebKit/534.1+ (KHTML, like Gecko) Version/6.0.0.337 Mobile Safari/534.1+'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (hp-tablet; Linux; hpwOS/3.0.0; U; en-US) AppleWebKit/534.6 (KHTML, like Gecko) wOSBrowser/233.70 Safari/534.6 TouchPad/1.0'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (SymbianOS/9.4; Series60/5.0 NokiaN97-1/20.0.019; Profile/MIDP-2.1 Configuration/CLDC-1.1) AppleWebKit/525 (KHTML, like Gecko) BrowserNG/7.1.18124'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/5.0 (compatible; MSIE 9.0; Windows Phone OS 7.5; Trident/5.0; IEMobile/9.0; HTC; Titan)'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'UCWEB7.0.2.37/28/999'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'NOKIA5700/ UCWEB7.0.2.37/28/999'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Openwave/ UCWEB7.0.2.37/28/999'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 6.0; ) Opera/UCWEB7.0.2.37/28/999'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'UCWEB7.0.2.37/28/999'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'NOKIA5700/ UCWEB7.0.2.37/28/999'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Openwave/ UCWEB7.0.2.37/28/999'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Mozilla/4.0 (compatible; MSIE 6.0; ) Opera/UCWEB7.0.2.37/28/999'")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("blockquote",[s("p",[t._v("注意：该 "),s("code",[t._v("USER_AGENT_LIST")]),t._v(" 变量名可以自定义，也可以写在其他 py 文件中，写在 settings.py 文件中，只是为了规范而已。")])]),t._v(" "),s("h4",{attrs:{id:"_2-定义下载中间件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-定义下载中间件"}},[t._v("#")]),t._v(" 2）定义下载中间件")]),t._v(" "),s("p",[t._v("在 "),s("code",[t._v("middlewares.py")]),t._v(" 文件中定义随机 UA 的下载中间件：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" random\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" Douban"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("settings "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" USER_AGENT_LIST\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("RandomUserAgentMiddleware")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("object")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("process_request")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" request"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# print(request.headers['User-Agent'])")]),t._v("\n        ua "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" random"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("USER_AGENT_LIST"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        request"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("headers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'User-Agent'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ua\n")])])]),s("h4",{attrs:{id:"_3-启用下载中间件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-启用下载中间件"}},[t._v("#")]),t._v(" 3）启用下载中间件")]),t._v(" "),s("p",[t._v("在 "),s("code",[t._v("settings.py")]),t._v(" 文件中配置开启自定义的下载中间件：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("DOWNLOADER_MIDDLEWARES "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Douban.middlewares.RandomUserAgentMiddleware'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("543")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("h3",{attrs:{id:"_4、实现-selenium-渲染的下载中间件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4、实现-selenium-渲染的下载中间件"}},[t._v("#")]),t._v(" 4、实现 Selenium 渲染的下载中间件")]),t._v(" "),s("p",[t._v("scrapy 的 Downloader 模块只会根据请求获取响应，但实际开发过程中，有些页面上的数据是通过 ajax 延迟加载出来的，Downloader 模块无法应对这种情况，这时就需要用到 Selenium 来处理这类请求，等页面渲染完成后，再把渲染好的页面返回给爬虫即可：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" selenium "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" webdriver\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" scrapy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("http "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" HtmlResponse\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("SeleniumMiddleware")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("object")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("driver "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" webdriver"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Chrome"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("process_request")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" request"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" spider"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        url "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" request"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("url\n\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 过滤需要使用selenium渲染的request")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'daydata'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("driver"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            time"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sleep"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("driver"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("page_source\n            self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("driver"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 返回HtmlResponse，请求不会达到Downloader，而是直接通过引擎交给爬虫")]),t._v("\n            response "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" HtmlResponse"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("url"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" body"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" encoding"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'utf-8'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" request"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("request"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" response\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__del__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("driver"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("quit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);